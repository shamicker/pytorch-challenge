{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning**\n",
    "inside the field of machine learning, using massive datasets, accelerated computing on GPU (graphics processing units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch** - open-source Python framework\n",
    "\n",
    "- from Facebook's AI Research team (FAIR)\n",
    "- used for developing deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**\n",
    "\n",
    "- imitates how a brain works\n",
    "- in the same way as in a brain\n",
    "    - the *input* nodes are one-way streets to a calculation node\n",
    "    - some decision happens there, and produces a one-way action to the output\n",
    "        - in a brain it happens or it doesn't, but here it should always happen but with different results\n",
    "\n",
    "\n",
    "- basically, it finds a boundary between different categories (like flower species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**\n",
    "- inputs; the data that is input into a neural network for it to study\n",
    "\n",
    "**Targets** \n",
    "- the desired outcomes\n",
    "- denoted by $y$\n",
    "\n",
    "**Prediction** \n",
    "- the output; the prediction of how items are categorized\n",
    "- aka **score**\n",
    "- aka **logits**\n",
    "- denoted by $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Boundaries**\n",
    "\n",
    "- a boundary that's linear\n",
    "- this is the simplest solution/output produced by a neural network (ie to differentiate between the target categories)\n",
    "- calculated with $Wx + b = 0$, where `W` is some weights, and `b` is a bias\n",
    "\n",
    "\n",
    "- there are higher dimensions: curves, planes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron**\n",
    "- a building block of neural networks\n",
    "- a visualisation of an equation into a small graph, where\n",
    "    - each input $x$ is in a node\n",
    "    - the weights $w$ are labeled on the *edges* (the arrows) of the input nodes\n",
    "    - the bias $b$ can either be in the calculation node or viewed as an *input* node\n",
    "        - usually the bias is considered a *weight* for an input of constant $1$, as in this image\n",
    "    - the calculation is also in a node\n",
    "    - the prediction is the return value\n",
    "\n",
    "![perceptron with summing equation](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/perceptron-summation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of Perceptrons as Logical Operators**\n",
    "\n",
    "An AND perceptron:\n",
    "![an AND perceptron visualized](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/and-perceptron.PNG)\n",
    "\n",
    "An OR perceptron:\n",
    "![an OR perceptron visualized](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/perceptron-or.PNG)\n",
    "\n",
    "\n",
    "An XOR perceptron, which is either or, but not both and not neither:\n",
    "![an XOR perceptron visualized](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/perceptron-xor.PNG)\n",
    "\n",
    "\n",
    "**2 different visuals for the XOR perceptron**\n",
    "![an XOR neural network](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/xor-neuralnetwork.PNG)\n",
    "\n",
    "![an XOR multi-layer perceptron](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/xor-multi-layer-perceptron.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron Algorithm**\n",
    "- adjusting the linear boundary by \"asking\" each data point if it's been classified correctly or incorrectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot Encoding**\n",
    "- the act of binary-izing multiple classes, or multiple outcomes\n",
    "    - in math terms, it's getting an **identity matrix**\n",
    "    - it's the matrix equivalent of $1$:\n",
    "        - you can multiply ANY matrix by an identity matrix and you'll get the original matrix back!\n",
    "        ![it's an identity matrix](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/Identity_matrix.PNG)\n",
    "        \n",
    "![one-hot encoding](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/one-hot_encoding.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Functions**\n",
    "- a way of presenting output; preparing it for the next step\n",
    "\n",
    "**Step Function**\n",
    "- the yes/no kind\n",
    "- for binary predictions\n",
    "- $Wx + b = 0$\n",
    "    - whether a data point is above or below the *linear boundary*\n",
    "    - aka if the data point output is positive or negative\n",
    "\n",
    "**Sigmoid Function**\n",
    "- probabilities of the outputs to be above or below the linear boundary (or correctly classified)\n",
    "- the high end approaches 1, the negative approaches 0, the middle is 50% (right at the linear boundary)\n",
    "- this can be for binary predictions or multi-layer networks\n",
    "- denoted as either of the following (they are the same thing):\n",
    "    - $\\sigma(x)$\n",
    "    - $1/(1 + e^{-x})$\n",
    "\n",
    "**Softmax Function**\n",
    "- for multi-class networks\n",
    "- this \"squishes\" the output, or **normalizes** it into a probability, where all the results must add up to 1\n",
    "- this also eliminates the problem of negative outputs (because $e^{negative-number}$ is positive)\n",
    "- the formula is $e^{each-output}/\\Sigma e^{each-output}$\n",
    "![probability of class i](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/softmax_function.PNG)\n",
    "\n",
    "**More Activation Functions**\n",
    "- **TanH**\n",
    "- **ReLU** is the most popular, simplest, and apparently extremely effective!\n",
    "![more activation functions](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/more_activation_functions.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Functions**\n",
    "- tell us how far we are from the solution; the distance from the goal\n",
    "- aka **loss**\n",
    "- aka **cost**\n",
    "\n",
    "There are different kinds of error functions:\n",
    "- discrete outputs (yes/no)\n",
    "- continuous outputs (probabilities)\n",
    "- log-loss (for 2 possible outputs)\n",
    "- cross entropy (for 3+ possible outputs)\n",
    "\n",
    "**Log Loss**\n",
    "\n",
    "**Cross Entropy**\n",
    "- a log-loss function\n",
    "- like everything in neural networks (I think?) we usually calculate with natural logs ($ln$), or base e\n",
    "- done to get all-positive numbers\n",
    "- we take the negative of the log of the probabilities\n",
    "\n",
    "\n",
    "The log of anything between $0$ and $1$ is a negative number, and the log of $1$ is $0$.\n",
    "\n",
    "We know that a high probability of success approaches 1 (100%).\n",
    "When we take the negative log of that probability, we get a low number. So think of this *negative log* as the **errors**.\n",
    "\n",
    "Because for each input, the outcome is either \"yes\" or \"no\"......?????\n",
    "\n",
    "**Mean Squared Loss**\n",
    "- often used in regression and binary classification problems\n",
    "![mean squared loss equation](https://raw.githubusercontent.com/shamicker/pytorch-challenge/master/images/mean_squared_loss_equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "- the negative slope of the loss function\n",
    "    - negative because the derivative returns the steepest ascent, but we want descent\n",
    "- is the resulting vector of partial derivatives, with respect to the weights\n",
    "    - points in the direction of fastest change (steepest ascent)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
